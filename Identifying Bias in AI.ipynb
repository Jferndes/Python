{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifier les biais en IA\n",
    "Ce tutoriel vous permet d'entraîner un modèle avec des données réelles et de s'entraîner à identifier les biais.\n",
    "### Introduction\n",
    "Civil Comments était une plateforme de commentaires communautaires conçue pour améliorer la qualité et la civilité des discussions en ligne. La plateforme introduisait un modèle de modération collective mêlant interaction humaine et apprentissage automatique. Avant de publier un commentaire, chaque utilisateur devait :\n",
    "  - Évaluer la courtoisie d’un autre commentaire (par un pouce vers le haut ou le bas) ;\n",
    "  - Juger s’il s’agissait d’un bon commentaire(« good / sorta / bad »).\n",
    "  \n",
    "Civil Comments fut une expérience pionnière de “Civic Tech”. En fermant en 2017, la plateforme a ouvert publiquement sa base de données (1,8 million de commentaires) afin de soutenir la recherche sur la toxicité du langage et les biais en ligne.\n",
    "\n",
    "Le jeu de données Civil Comments, maintenu depuis par Jigsaw et Google, est aujourd’hui une référence dans les études de détection automatique de propos toxiques et d’analyse de polarisation numérique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample toxic comment: Too dumb to even answer.\n",
      "Sample not-toxic comment: No they aren't.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Get the same results each time\n",
    "np.random.seed(0)\n",
    "\n",
    "# Load the training data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "comments = data[\"comment_text\"]\n",
    "target = (data[\"target\"]>0.7).astype(int)\n",
    "\n",
    "# Break into training and test sets\n",
    "comments_train, comments_test, y_train, y_test = train_test_split(comments, target, test_size=0.30, stratify=target)\n",
    "\n",
    "# Get vocabulary from training data\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(comments_train)\n",
    "\n",
    "# Get word counts for training and test sets\n",
    "X_train = vectorizer.transform(comments_train)\n",
    "X_test = vectorizer.transform(comments_test)\n",
    "\n",
    "# Preview the dataset\n",
    "data.head()\n",
    "print(\"Sample toxic comment:\", comments_train.iloc[22])\n",
    "print(\"Sample not-toxic comment:\", comments_train.iloc[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exécutez la cellule de code suivante afin d'utiliser les données pour entraîner un modèle simple. Le résultat montre la précision du modèle sur certaines données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9304755967877966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a model and evaluate performance on test dataset\n",
    "classifier = LogisticRegression(max_iter=2000)\n",
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", score)\n",
    "\n",
    "# Function to classify any string\n",
    "def classify_string(string, investigate=False):\n",
    "    prediction = classifier.predict(vectorizer.transform([string]))[0]\n",
    "    if prediction == 0:\n",
    "        print(\"NOT TOXIC:\", string)\n",
    "    else:\n",
    "        print(\"TOXIC:\", string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environ 93 % des commentaires dans les données de test sont classés correctement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Essayer le modèle\n",
    "Utiliser la cellule suivante pour écrire vos propres commentaires et les transmettre au modèle, et vérifier (ou pas) que le modèle les classe comme toxiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT TOXIC: I love apples\n",
      "TOXIC: This guy is a perfect asshole\n",
      "NOT TOXIC: I been there, it's awesome\n",
      "TOXIC: The referee is shit!\n",
      "TOXIC: You are a fucking cunt, lady\n"
     ]
    }
   ],
   "source": [
    "# Comment to pass through the model\n",
    "my_comments = [\n",
    "    \"I love apples\", \n",
    "    \"This guy is a perfect asshole\", \n",
    "    \"I been there, it's awesome\", \n",
    "    \"The referee is shit!\", \n",
    "    \"You are a fucking cunt, lady\"\n",
    "]\n",
    "results = [classify_string(c) for c in my_comments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les commentaires testés, nous allons comprendre comment le modèle prend des décisions dans la cellule de code suivante.\n",
    "\n",
    "Le modèle attribue un coefficient à chacun des quelque 58 000 mots, les coefficients les plus élevés indiquant les mots jugés les plus toxiques par le modèle. La cellule de code affiche les dix mots considérés comme les plus toxiques, ainsi que leurs coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20745</th>\n",
       "      <td>fools</td>\n",
       "      <td>6.317838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34211</th>\n",
       "      <td>moron</td>\n",
       "      <td>6.357154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16844</th>\n",
       "      <td>dumb</td>\n",
       "      <td>6.399255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12907</th>\n",
       "      <td>crap</td>\n",
       "      <td>6.533120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38317</th>\n",
       "      <td>pathetic</td>\n",
       "      <td>6.603762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25850</th>\n",
       "      <td>idiotic</td>\n",
       "      <td>7.061440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49802</th>\n",
       "      <td>stupidity</td>\n",
       "      <td>7.617183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25858</th>\n",
       "      <td>idiots</td>\n",
       "      <td>8.694790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25847</th>\n",
       "      <td>idiot</td>\n",
       "      <td>8.701940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49789</th>\n",
       "      <td>stupid</td>\n",
       "      <td>9.318078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word     coeff\n",
       "20745      fools  6.317838\n",
       "34211      moron  6.357154\n",
       "16844       dumb  6.399255\n",
       "12907       crap  6.533120\n",
       "38317   pathetic  6.603762\n",
       "25850    idiotic  7.061440\n",
       "49802  stupidity  7.617183\n",
       "25858     idiots  8.694790\n",
       "25847      idiot  8.701940\n",
       "49789     stupid  9.318078"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients = pd.DataFrame({\"word\": sorted(list(vectorizer.vocabulary_.keys())), \"coeff\": classifier.coef_[0]})\n",
    "coefficients.sort_values(by=['coeff']).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Mots les plus toxiques\n",
    "Regardez les mots les plus toxiques de la cellule de code ci-dessus. Êtes-vous surpris d'en voir certains ? Y a-t-il des mots qui ne devraient pas figurer dans la liste ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Analyse plus approfondie\n",
    "Examinons de plus près comment le modèle classe les commentaires.\n",
    "   1. Commencez par exécuter la cellule de code telle quelle pour classer le commentaire « I have a christian friend ». Vous devriez constater qu'il a été classé comme « NON TOXIQUE ». De plus, vous pouvez voir les scores attribués à certains mots. Notez que tous les mots du commentaire n'apparaîtront probablement pas.\n",
    "   2. Ensuite, essayez « I have a muslim friend », puis « I have a white friend » et « I have a black friend ».\n",
    "N'hésitez pas à tester d'autres commentaires pour voir comment le modèle les classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT TOXIC: I have a christian friend\n",
      "TOXIC: I have a muslim friend\n",
      "NOT TOXIC: I have a white friend\n",
      "TOXIC: I have a black friend\n",
      "NOT TOXIC: Today, I saw a black friend\n",
      "TOXIC: Pigs are great animals\n",
      "NOT TOXIC: Cows are great animals\n",
      "TOXIC: Rats are tiny animals and they are often misunderstood.\n",
      "TOXIC: You are a rat\n",
      "TOXIC: Fuck\n"
     ]
    }
   ],
   "source": [
    "# Set the value of new_comment\n",
    "new_comments = [\n",
    "    \"I have a christian friend\", \n",
    "    \"I have a muslim friend\", \n",
    "    \"I have a white friend\", \n",
    "    \"I have a black friend\",\n",
    "    \"Today, I saw a black friend\",\n",
    "    \"Pigs are great animals\",\n",
    "    \"Cows are great animals\",\n",
    "    \"Rats are tiny animals and they are often misunderstood.\",\n",
    "    \"You are a rat\",\n",
    "    \"Fuck\"\n",
    "]\n",
    "\n",
    "results = [(c, classify_string(c), coefficients[coefficients.word.isin(c.split())])for c in new_comments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Identifier les biais\n",
    "Voyez-vous des signes de biais potentiel dans le modèle ? Dans la cellule de code ci-dessus,\n",
    "- Comment le modèle a-t-il classé « I have a christan friend » et « I have a muslim friend » ?\n",
    "- Comment a-t-il classé « I have a white friend » et « I have a black friend » ?\n",
    "Une fois la réponse obtenue, exécutez la cellule de code suivante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Testez votre compréhension \n",
    "Nous allons nous éloigner des données du concours Jigsaw et envisager un scénario similaire (mais hypothétique !) où vous travaillez avec un ensemble de données de commentaires en ligne pour entraîner un modèle capable de classer les commentaires comme toxiques.\n",
    "\n",
    "Vous remarquez que les commentaires faisant référence à l'islam sont plus susceptibles d'être toxiques que ceux faisant référence à d'autres religions, car la communauté en ligne est islamophobe. Quel type de biais cela peut-il introduire dans votre modèle ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Testez votre compréhension (suite)\n",
    "Nous reprenons le même scénario hypothétique : vous essayez d'entraîner un modèle pour classer les commentaires en ligne comme toxiques.\n",
    "\n",
    "Vous prenez tous les commentaires qui ne sont pas déjà en anglais et vous les traduisez en anglais avec un outil distinct. Ensuite, vous traitez tous les messages comme s'ils avaient été initialement rédigés en anglais. De quel type de biais votre modèle souffrira-t-il ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Testez votre compréhension (suite et fin)\n",
    "Nous reprenons le même scénario hypothétique : vous essayez d'entraîner un modèle pour classer les commentaires en ligne comme toxiques.\n",
    "\n",
    "L'ensemble de données utilisé pour entraîner le modèle contient principalement des commentaires d'utilisateurs basés au Royaume-Uni.\n",
    "\n",
    "Après avoir entraîné un modèle, vous évaluez ses performances avec un autre ensemble de données de commentaires, également principalement d'utilisateurs basés au Royaume-Uni, et les performances sont excellentes ! Vous le déployez pour une entreprise basée en Australie, et ses performances sont médiocres en raison des différences entre l'anglais britannique et l'anglais australien. De quels types de biais le modèle souffre-t-il ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
